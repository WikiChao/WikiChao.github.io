<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Chao Huang's Homepage</title>
  
  <meta name="author" content="Chao Huang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/website_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td halign="center">
          <p align="center">
              <font size="6">Chao Huang</font>
          </p>
      </td>
    </tr>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <!-- <p style="text-align:center">
                <name>Chao Huang</name>
              </p> -->
              <p>
                I am a fifth-year PhD candidate in the Department of Computer Science at the University of Rochester, advised by <a href="https://www.cs.rochester.edu/~cxu22/">Prof. Chenliang Xu</a>. Previously, I spent one wonderful year as a research assistant at the Chinese University of Hong Kong, working with <a href="http://www.cse.cuhk.edu.hk/~cwfu/">Prof. Chi-Wing Fu</a> on 3D vision. I received my B.Eng. from ESE Department, Nanjing University in 2019. In my undergrad, I worked with <a href="https://scholar.google.com.sg/citations?hl=en&user=78KxtRMAAAAJ">Prof. Zhan Ma</a> on image compression.
                <br>
                <br>
                I am working on multimodal learning and generation. Recently, I am particularly interested in how to leverage the power of large language models (LLMs) to enhance multimodal understanding and generation.
              </p>
              <p><b>Research opportunities:</b> I am open to collaborating on research projects. Shoot me an email if you are insterested. 
              
                <div class="job-alert" style="background-color: #ffeeee; border: 1px solid #ff6a5c; padding: 10px; border-radius: 5px; margin: 15px 0;">
                  <p style="color: #cc0000; margin: 0; font-weight: 500;">‚úâÔ∏è I'm currently seeking full-time opportunities. Please feel free to reach out if you have any openings!</p>
                </div>


              <!-- Recently I am looking for collaborators on the following topics:
                <ul>
                  <li>Audio-Visual Scene Understanding</li>
                  <li>Language-Guided Vision and Audio</li>
                  <li>Acoustic Scene Understanding</li>
                  <li>3D Vision and Graphics</li>
                </ul> -->
              </p>

              <p style="text-align:center">
                <a href="mailto:chuang65@cs.rochester.edu">Email</a> &nbsp/&nbsp
                <a href="data/Chao_Huang_s_CV.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=5yYP5RIAAAAJ&hl=en">Google Scholar</a> 
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <!-- <a href="https://github.com/jonbarron/">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/avatar.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/avatar.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
          </td>
        </tr>
      </tbody></table>
      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
      </tbody></table> -->
      <!-- <ul>
        <li><strong>[March 2023]</strong> I will be joining Meta Reality Labs Research Pitt this summer for internship! </li>
        <li><strong>[March 2023]</strong> I will be joining Meta Reality Labs Research Pitt this summer for internship! </li>
      </ul> -->
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <!-- <colgroup>
            <col width="15%">
            <col width="80%">
        </colgroup> -->
        <tbody>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[09/2025]</strong></td>
            <td style="width: 75%;">Three papers accepted to <a href="https://neurips.cc/">NeurIPS 2025</a> and one paper accepted to <a href="https://link.springer.com/journal/11263">IJCV!</a>
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[08/2025]</strong></td>
            <td style="width: 75%;">Selected for ICCV 2025 Doctoral Consortium!
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[06/2025]</strong></td>
            <td style="width: 75%;">One paper accepted to <a href="https://iccv.thecvf.com/">ICCV 2025</a>! See you in Hawaii.
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[04/2025]</strong></td>
            <td style="width: 75%;">I am co-organizing the <a href="https://t2fm-ws.github.io/T2FM-ICCV25/index.html">üîí TrustFM: Workshop on Trustworthy Foundation Models</a> @ ICCV 2025!
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[04/2025]</strong></td>
            <td style="width: 75%;"><a href="https://arxiv.org/pdf/2312.17432v2.pdf"><papertitle>Video Understanding with Large Language Models: A Survey</papertitle></a> is accepted to IEEE TCSVT!
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[02/2025]</strong></td>
            <td style="width: 75%;">Two papers accepted to <a href="https://cvpr.thecvf.com/">CVPR 2025</a>! See you in Nashville &#127928;.
            </td>
          </tr>
          <tr> 
            <td valign="top" align="center" style="width: 15%;"><strong>[12/2024]</strong></td>
            <td style="width: 75%;"> üèÜ DAVIS won the <font color="#ff6a5c"><strong>ACCV 2024 Best Paper Award, Honorable Mention!</strong></font>
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[09/2024]</strong></td>
            <td style="width: 75%;">Two papers accepted to <a href="https://accv2024.org/">ACCV 2024</a> with DAVIS as <font color="#ff6a5c"><strong>Oral</strong></font> presentation. See you in Hanoi, Vietnam üçú.
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[07/2024]</strong></td>
            <td style="width: 75%;">Acoustic Primitives is accepted to <a href="https://eccv2024.ecva.net/">ECCV 2024</a>! See you in Milan &#9962.
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[05/2024]</strong></td>
            <td style="width: 75%;">I have rejoined <a href="https://about.meta.com/realitylabs/">Meta Reality Labs</a> as a summer research intern, this time based in the UK.
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[09/2023]</strong></td>
            <td style="width: 75%;">One paper accepted to <a href="https://nips.cc/">NeurIPS 2023</a>!
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[06/2023]</strong></td>
            <td style="width: 75%;">Invited paper talk at <a href="https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/">Joint International 3rd Ego4D and 11th EPIC Workshop
            </a> @ CVPR 2023.
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[03/2023]</strong></td>
            <td style="width: 75%;">I will be joining <a href="https://about.meta.com/realitylabs/">Meta Reality Labs Pittsburgh</a> for summer internship!
            </td>
          </tr>
          <tr>
            <td valign="top" align="center" style="width: 15%;"><strong>[02/2023]</strong></td>
            <td style="width: 75%;">One paper accepted to <a href="https://cvpr2023.thecvf.com/Conferences/2023">CVPR 2023</a>!
            </td>
          </tr>
        </tbody>
    </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/drift.png" alt="DRIFT" width="210" height="80">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2510.15050"><papertitle>DRIFT: Directional Reasoning Injection for Fine-Tuning MLLMs</papertitle></a>
              <br>
              <strong>Chao Huang</strong>, <a href="https://zhangaipi.github.io/">Zeliang Zhang</a>, <a href="https://joellliu.github.io/">Jiang Liu</a>, <a href="https://scholar.google.com/citations?user=XoY8DLwAAAAJ&hl=en">Ximeng Sun</a>, <a href="https://jialianwu.com/">Jialian Wu</a>, <a href="https://www.xiaodongyu.me/">Xiaodong Yu</a>, <a href="https://zewang95.github.io/">Ze Wang</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>, <a href="https://scholar.google.com/citations?user=bX1YILcAAAAJ&hl=en">Emad Barsoum</a>, <a href="https://zicliu.wixsite.com/mysite">Zicheng Liu</a>
              <br>
              <em>arXiv preprint</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2510.15050">Paper</a> /
              <a href="https://wikichao.github.io/DRIFT/">Project Page</a> / 
              <a href="https://github.com/WikiChao/DRIFT">Code</a>
              <p>DRIFT transfers reasoning from DeepSeek-R1 into QwenVL via gradient-space guidance, improving multimodal reasoning without destabilizing alignment or expensive RL.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/xmodbench.png" alt="XModBench" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://xingruiwang.github.io/projects/XModBench"><papertitle>XModBench: Benchmarking Cross-Modal Capabilities and Consistency in Omni-Language Models</papertitle></a>
              <br>
              <a href="https://xingruiwang.github.io/">Xingrui Wang</a>, <a href="https://joellliu.github.io/">Jiang Liu</a>, <strong>Chao Huang</strong>, <a href="https://www.xiaodongyu.me/">Xiaodong Yu</a>, <a href="https://zewang95.github.io/">Ze Wang</a>, <a href="https://sunxm2357.github.io/">Ximeng Sun</a>, <a href="https://jialianwu.com/">Jialian Wu</a>, <a href="https://www.cs.jhu.edu/~ayuille/">Alan Yuille</a>, <a href="https://www.microsoft.com/en-us/research/people/">Emad Barsoum</a>, <a href="https://zicliu.wixsite.com/mysite">Zicheng Liu</a>
              <br>
              <em>arXiv preprint</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2510.15148">Paper</a> / 
              <a href="https://xingruiwang.github.io/projects/XModBench">Project Page</a> /
			  <a href="https://github.com/XingruiWang/XModBench">Code</a> /
			  <a href="https://huggingface.co/datasets/RyanWW/XModBench">Data</a>
              <p>A benchmark for evaluating cross-modal capabilities and consistency in omni-language models.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/davis_flow.png" alt="davis-flow" width="210" height="140">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2505.23625"><papertitle> High-Quality Sound Separation Across Diverse Categories via
                  Visually-Guided Generative Modeling
                </papertitle></a>
              <br>
              <strong>Chao Huang</strong>, <a href="https://liangsusan-git.github.io/">Susan Liang</a>, <a href="http://www.yapengtian.com/">Yapeng Tian</a>, <a href="https://anuragkr90.github.io/">Anurag Kumar</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>IJCV</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2509.22063">Paper</a> / 
              <a href="https://wikichao.github.io/data/projects/DAVIS/">Project Page</a> / <a href="https://github.com/WikiChao/DAVIS">Code</a>
              <p>How generative models can improve sound separation across diverse categories with visually-guided training.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/zerossep_logo.png" alt="diffusion" width="210" height="140">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2505.23625"><papertitle> ZeroSep: Separate Anything in Audio with Zero Training
                </papertitle></a>
              <br>
              <strong>Chao Huang</strong>, <a href="#">Yuesheng Ma</a>, <a href="#">Junxuan Huang</a>, <a href="https://liangsusan-git.github.io/">Susan Liang</a>, <a href="https://yunlong10.github.io/">Yunlong Tang</a>, <a href="https://scholar.google.com/citations?user=ZyCYhUkAAAAJ&hl=en">Jing Bi</a>, <a href="https://scholar.google.com/citations?user=ZDNBJ6UAAAAJ&hl=zh-CN">Wenqiang Liu</a>, <a href="https://nima.ee.columbia.edu/">Nima Mesgarani</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>NeurIPS, 2025</em>
              <br>
              <a href="https://arxiv.org/pdf/2505.23625">Paper</a> / 
              <a href="https://wikichao.github.io/ZeroSep/">Project Page</a> / 
              <a href="https://github.com/WikiChao/ZeroSep">Code</a>
              <p>No fine-tuning, no task-specific data, just latent inversion + text-conditioned denoising to isolate any sound you describe.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vit_attack.png" alt="vit_attack" width="210" height="140">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2504.10804"><papertitle> Harnessing the Computation Redundancy in ViTs to Boost Adversarial Transferability
                </papertitle></a>
              <br>
              <a href="#">Jiani Liu*</a>, <a href="#">Zhiyuan Wang*</a>, <a href="https://zhangaipi.github.io/">Zeliang Zhang*</a>, <strong>Chao Huang</strong>, <a href="https://liangsusan-git.github.io/">Susan Liang</a>, <a href="https://yunlong10.github.io/">Yunlong Tang</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>.
              <br>
              <em>NeurIPS, 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2504.10804">Paper</a>
              <p>We propose a bag of tricks to boost the adversarial transferability of ViT-based attacks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mmperspective-480.webp" alt="mmperspective" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2505.20426"><papertitle>MMPerspective: Do MLLMs Understand Perspective? A Comprehensive Benchmark for Perspective Perception, Reasoning, and Robustness
                </papertitle></a>
              <br>
              <a href="https://yunlong10.github.io/">Yunlong Tang</a>, <a href="https://andypinxinliu.github.io/">Pinxin Liu</a>, <a href="https://fmmarkmq.github.io/">Mingqian Feng</a>, <a href="#">Zhangyun Tan</a>, <a href="#">Rui Mao</a>, <strong>Chao Huang</strong>, <a href="https://scholar.google.com/citations?user=ZyCYhUkAAAAJ&hl=en">Jing Bi</a>, <a href="#">Yunzhong Xiao</a>, <a href="https://liangsusan-git.github.io/">Susan Liang</a>, <a href="https://hanghuacs.notion.site/">Hang Hua</a>, and <a href="https://alivosoughi.com/">Ali Vosoughi</a>, <a href="https://songluchuan.github.io/">Luchuan Song</a>, <a href="https://zhangaipi.github.io/">Zeliang Zhang</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>NeurIPS, 2025, Datasets and Benchmarks</em>
              <br>
              <a href="https://arxiv.org/abs/2505.20426">Paper</a> / 
              <a href="https://yunlong10.github.io/MMPerspective/">Project Page</a> / 
              <a href="https://github.com/yunlong10/MMPerspective">Code</a>
              <p>Introducing MMPerspective, a comprehensive benchmark for MLLMs on perspective understanding.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pi_avas.png" alt="piavas" width="210" height="140">
            </td>
            <td width="75%" valign="middle">
                <a href="#"><papertitle>&pi;-AVAS: Can Physics-Integrated Audio-Visual Modeling Boost Neural Acoustic Synthesis?
                </papertitle></a>
              <br>
             <a href="https://liangsusan-git.github.io/">Susan Liang</a>,  <strong>Chao Huang</strong>, <a href="https://yunlong10.github.io/">Yunlong Tang</a>, <a href="https://zhangaipi.github.io/">Zeliang Zhang</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>ICCV, 2025</em>
              <br>
              <p>&pi;-AVAS is a two-stage framework that combines physics-based vision-guided audio simulation for generalization with flow-matching audio refinement for realism.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fresca.jpg" alt="diffusion" width="210" height="110">
            </td>
            <td width="75%" valign="middle">
                <a href="/"><papertitle>FreSca: Scaling in Frequency Space Enhances Diffusion Models
                </papertitle></a>
              <br>
              <strong>Chao Huang</strong>, <a href="https://liangsusan-git.github.io/">Susan Liang</a>, <a href="https://yunlong10.github.io/">Yunlong Tang</a>, <a href="https://limacv.github.io/homepage/">Li Ma</a>, <a href="http://www.yapengtian.com/">Yapeng Tian</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>CVPR GMCV, 2025</em>
              <br>
              <a href="https://arxiv.org/pdf/2504.02154">Paper</a> / 
              <a href="https://wikichao.github.io/FreSca/">Project Page</a> / 
              <a href="https://github.com/WikiChao/FreSca">Code</a>
              <p>Where and why you should care about frequency space in diffusion models.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/visah.png" alt="diffusion" width="210" height="150">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2505.12154"><papertitle>Learning to Highlight Audio by Watching Movies
                </papertitle></a>
              <br>
              <strong>Chao Huang</strong>, <a href="https://ruohangao.github.io/">Ruohan Gao</a>,  J. M. F. Tsang, Jan Kurcius, Cagdas Bilen, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>, <a href="https://anuragkr90.github.io/">Anurag Kumar</a>, <a href="https://sanjeelparekh.github.io/">Sanjeel Parekh</a>
              <br>
              <em>CVPR, 2025</em>
              <br>
              <a href="https://arxiv.org/pdf/2505.12154">Paper</a> / 
              <a href="https://wikichao.github.io/VisAH/">Project Page</a> / 
              <a href="https://github.com/WikiChao/VisAH">Code</a> / 
              <a href="https://drive.google.com/file/d/1lVqr7zBNaI1AupLz0X7dIWbiC8WULWP4/view">Dataset</a>
              <p>We learn from movies to transform audio to deliver appropriate highlighting effects guided by the accompanying video.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vidllm_survey.png" alt="vidllm" width="210" height="130">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2312.17432v2.pdf"><papertitle> Video Understanding with Large Language Models: A Survey</papertitle></a>
              <br>
              Yunlong Tang*, ... , <b>Chao Huang</b>, ... , Ping Luo, Jiebo Luo, Chenliang Xu
              <br>
              <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2312.17432v2.pdf">Paper</a> / 
              <a href="https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding">Project Page</a>
              <p>A survey on the recent Large Language Models for video understanding.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vidcomposition.png" alt="diffusion" width="210" height="140">
            </td>
            <td width="75%" valign="middle">
                <a href="/"><papertitle> 
                  VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?
                </papertitle></a>
              <br>
              Yunlong Tang*, Junjia Guo*, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, <b>Chao Huang</b>, Jing Bi, Zeliang Zhang, and Pooyan Fazli, Chenliang Xu
              <br>
              <em>CVPR, 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2411.10979">Paper</a> / 
              <a href="https://yunlong10.github.io/VidComposition/">Project Page</a> / 
              <a href="https://github.com/yunlong10/VidComposition">Code</a>
              <p>We introduce VidComposition, a benchmark designed to assess MLLMs' understanding of video compositions</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/scalingconcept.png" alt="diffusion" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
                <a href="/"><papertitle>Scaling Concept with Text-Guided Diffusion Models
                </papertitle></a>
              <br>
              <strong>Chao Huang</strong>, <a href="https://liangsusan-git.github.io/">Susan Liang</a>, <a href="https://yunlong10.github.io/">Yunlong Tang</a>, <a href="http://www.yapengtian.com/">Yapeng Tian</a>, <a href="https://anuragkr90.github.io/">Anurag Kumar</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>arXiv preprint, 2024</em>
              <br>
              <a href="https://arxiv.org/pdf/2410.24151">Paper</a> / 
              <a href="https://wikichao.github.io/ScalingConcept/">Project Page</a> / 
              <a href="https://github.com/WikiChao/ScalingConcept">Code</a>
              <p>We use pretrained text-guided diffusion models
                to scale up/down concepts in image/audio.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/davis.png" alt="davis" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2308.00122.pdf"><papertitle>DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models</papertitle></a>
              <br>
              <strong>Chao Huang</strong>, <a href="https://liangsusan-git.github.io/">Susan Liang</a>, <a href="http://www.yapengtian.com/">Yapeng Tian</a>, <a href="https://anuragkr90.github.io/">Anurag Kumar</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>ACCV</em>, 2024  <font color="#ff6a5c"><strong>üèÜ Best Paper Award, Honorable Mention</strong></font>
              <br>
              <a href="https://arxiv.org/pdf/2308.00122.pdf">Paper</a> / 
              <a href="https://wikichao.github.io/data/projects/DAVIS/">Project Page</a> / <a href="https://github.com/WikiChao/DAVIS">Code</a>
              <p>A new take on the audio-visual separation problem with the recent generative diffusion models.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/OAVE.svg" alt="OAVE" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2410.07463"><papertitle>Language-Guided Joint Audio-Visual Editing Via One-Shot Adaptation</papertitle></a>
              <br>
              <a href="https://liangsusan-git.github.io/">Susan Liang</a>, <strong>Chao Huang</strong>, <a href="http://www.yapengtian.com/">Yapeng Tian</a>, <a href="https://anuragkr90.github.io/">Anurag Kumar</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>ACCV</em>, 2024 
              <br>
              <a href="https://arxiv.org/pdf/2410.07463">Paper</a> / 
              <a href="https://liangsusan-git.github.io/project/avedit/">Project Page</a> /
              <a href="https://github.com/liangsusan-git/OAVE">Dataset</a>
              <p>We achieve joint audio-visual editing under language guidance.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="Acoustic-Primitives/data/speech.png" alt="acoustic-primitives" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
                <a href="/"><papertitle>Modeling and Driving Human Body Soundfields through Acoustic Primitives</papertitle></a>
              <br>
              <strong>Chao Huang</strong>, <a href="https://scholar.google.com/citations?hl=en&user=cyAYD3UAAAAJ&view_op=list_works&sortby=pubdate">Dejan Markovic</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>, <a href="https://alexanderrichard.github.io/">Alexander Richard
              </a>
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2407.13083">Paper</a> / 
              <a href="https://wikichao.github.io/Acoustic-Primitives/">Project Page</a>
              <p>Thinking of the equivalent of 3D Gaussian Splatting and volumetric primitives for the human body soundfield? Here, we introduce Acoustic Primitives.</p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/av_nerf.png" alt="av_nerf" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2302.02088"><papertitle>AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis</papertitle></a>
              <br>
              <a href="https://liangsusan-git.github.io/">Susan Liang</a>, <strong>Chao Huang</strong>, <a href="http://www.yapengtian.com/">Yapeng Tian</a>, <a href="https://anuragkr90.github.io/">Anurag Kumar</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>NeurIPS</em>, 2023
              <br>
              <a href="https://arxiv.org/pdf/2302.02088.pdf">Paper</a> / 
              <a href="https://liangsusan-git.github.io/project/avnerf/">Project Page</a> /
              <a href="https://github.com/liangsusan-git/AV-NeRF/">Code</a>
              <p>We propose a novel method of synthesizing real-world audio-visual scenes at novel positions and directions.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ego_av_loc.png" alt="ego_av_loc" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2303.13471"><papertitle>Egocentric Audio-Visual Object
                  Localization</papertitle></a>
              <br>
              <strong>Chao Huang</strong>, <a href="http://www.yapengtian.com/">Yapeng Tian</a>, <a href="https://anuragkr90.github.io/">Anurag Kumar</a>, <a href="https://www.cs.rochester.edu/~cxu22/">Chenliang Xu</a>
              <br>
              <em>CVPR</em>, 2023
              <br>              
              <a href="https://arxiv.org/abs/2303.13471">Paper</a> / 
              <a href="https://github.com/WikiChao/Ego-AV-Loc/">Code</a>
              <p>We explore the problem of sound source visual localization in egocentric videos, propose a new localization method and establish a benchmark for evaluation.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/point_denoise.png" alt="point_denoise" width="210" height="120">
            </td>
            <td width="75%" valign="middle"><a href="https://arxiv.org/pdf/2003.06631"><papertitle>Non-Local Part-Aware Point Cloud Denoising</papertitle></a>
                
              <br>
              <strong>Chao Huang*</strong>, <a href="https://liruihui.github.io/">Ruihui Li*</a>, <a href="https://nini-lxz.github.io/">Xianzhi Li</a>, <a href="https://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>
              <br>
              <em>arXiv preprint</em>, 2020
              <!-- <br>
              <a href="https://arxiv.org/pdf/2003.06631">Paper</a> -->
              <p>A non-local attention based method for point cloud denoising in both synthetic and real scenes.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/extreme_compression.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle"><a href="https://arxiv.org/pdf/1904.03851.pdf"><papertitle>Extreme Image Compression via Multiscale Autoencoders With Generative Adversarial Optimization</papertitle></a>
                
              <br>
              <strong>Chao Huang</strong>, <a href="https://scholar.google.com/citations?user=nYENH40AAAAJ&hl=zh-CN">Haojie Liu</a>, <a href="https://scholar.google.com/citations?user=HfBUtiIAAAAJ&hl=zh-CN">Tong Chen</a>, <a href="https://www.researchgate.net/profile/Qiu-Shen-3">Qiu Shen</a>, <a href="https://vision.nju.edu.cn/fc/d3/c29470a457939/page.htm">Zhan Ma</a>
              <br>
              <em>IEEE Visual Communications and Image Processing (VCIP)</em>, 2019 &nbsp <font color="#ff6a5c"><strong>(Oral Presentation)</strong></font>
              <!-- <br>
              <a href="https://arxiv.org/pdf/1904.03851.pdf">Paper</a> -->
              <p>An image compression system under extreme condition, <em>e.g.,</em> < 0.05 bits per pixel (bpp). </p>
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Education</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="text-align: center;padding:20px;width:25%;vertical-align:middle"><img src="images/uofr-logo-shield.png" width="120" ></td>
            <td width="75%" valign="center">
              <strong>University of Rochester</strong>, NY, USA
              <br>
              Ph.D. in Computer Science
              <br>
              Jan. 2021 - Present
              <br> 
              Advisor: <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
            </td>
          </tr>
          <tr>
            <td style="text-align: center;padding:20px;width:25%;vertical-align:middle"><img src="images/NJU_logo.jpg"  width="100"></td>
            <td width="75%" valign="center">
              <strong>Nanjing University</strong>, Nanjing, China
              <br>
              B.Eng in Electronic Science and Engineering
              <br>
              Sept. 2015 - Jun. 2019
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="text-align: center;padding:20px;width:25%;vertical-align:middle"><img src="images/AMD_Logo.svg" width="120" ></td>
            <td width="75%" valign="center">
              <strong>AMD Research</strong>, Remote
              <br>
              Research Scientist Intern
              <br>
              May. 2025 - Aug. 2025
              <br> 
              Mentor: Jiang Liu, Zicheng Liu
            </td>
          </tr>

          <tr>
            <td style="text-align: center;padding:20px;width:25%;vertical-align:middle"><img src="images/Reality_Labs_logo.png" width="120" ></td>
            <td width="75%" valign="center">
              <strong>Meta Reality Labs Research, Meta</strong>, Cambridge, UK
              <br>
              Research Scientist Intern
              <br>
              May. 2024 - Aug. 2024
              <br> 
              Mentor: <a href="https://sanjeelparekh.github.io/">Sanjeel Parekh
              </a>, <a href="https://ruohangao.github.io/">Ruohan Gao</a>, <a href="https://anuragkr90.github.io/">Anurag Kumar</a>
            </td>
          </tr>

          <tr>
            <td style="text-align: center;padding:20px;width:25%;vertical-align:middle"><img src="images/Reality_Labs_logo.png" width="120" ></td>
            <td width="75%" valign="center">
              <strong>Codec Avatars Lab, Meta</strong>, Pittsburgh
              <br>
              Research Scientist Intern
              <br>
              May. 2023 - Nov. 2023
              <br> 
              Mentor: <a href="https://scholar.google.com/citations?hl=en&user=cyAYD3UAAAAJ&view_op=list_works&sortby=pubdate">Dejan Markovic
              </a>, <a href="https://alexanderrichard.github.io/">Alexander Richard
              </a>
            </td>
          </tr>
          

          <tr>
            <td style="text-align: center;padding:20px;width:25%;vertical-align:middle"><img src="images/CUHK_logo.png" width="120" ></td>
            <td width="75%" valign="center">
              <strong>The Chinese University of Hong Kong</strong>, Shatin, Hong Kong
              <br>
              Research Assistant
              <br>
              Jul. 2019 - Dec. 2020
              <br> 
              Advisor: <a href="http://www.cse.cuhk.edu.hk/~cwfu/">Chi-Wing Fu</a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Awards &amp; Honors</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="width:25%;vertical-align:top"><strong>2025</strong></td>
            <td width="75%" valign="top">
              <span>ICCV 2025 Doctoral Consortium ‚Äî <font color="#ff6a5c"><strong>Selected Participant</strong></font></span>
            </td>
          </tr>
          <tr>
            <td style="width:25%;vertical-align:top"><strong>2024</strong></td>
            <td width="75%" valign="top">
              ACCV 2024 ‚Äî <font color="#ff6a5c"><strong>Best Paper Award, Honorable Mention</strong></font> for ‚ÄúDAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models‚Äù
            </td>
          </tr>
        </tbody></table>

        <!-- New Service Section -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Professional Service</heading>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <!-- Workshop Organization -->
          <tr>
            <td style="width:25%;vertical-align:top"><strong>Workshop Organization</strong></td>
            <td width="75%" valign="top">
              <ul style="margin:0; padding-left:18px;">
                <li>
                  <a href="https://t2fm-ws.github.io/T2FM-ICCV25/index.html">TrustFM: Workshop on Trustworthy Foundation Models</a>, ICCV&nbsp;2025 ‚Äî Organizer
                </li>
              </ul>
            </td>
          </tr>

          <!-- Conference Reviewing -->
          <tr>
            <td style="width:25%;vertical-align:top"><strong>Conference Reviewing</strong></td>
            <td width="75%" valign="top">
              <ul style="margin:0; padding-left:18px;">
                <li>CVPR (2023‚Äì2025)</li>
                <li>ICCV (2025)</li>
                <li>AAAI (2023‚Äì2025)</li>
                <li>ACM MM (2023‚Äì2025)</li>
                <li>NeurIPS (2025)</li>
              </ul>
            </td>
          </tr>

          <!-- Journal Reviewing -->
          <tr>
            <td style="width:25%;vertical-align:top"><strong>Journal Reviewing</strong></td>
            <td width="75%" valign="top">
              <ul style="margin:0; padding-left:18px;">
                <li>IEEE Transactions on Multimedia (TMM)</li>
                <li>IEEE Transactions on Image Processing (TIP)</li>
                <li>SIGGRAPH (ACM)</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px;text-align: center;">
              <br>
              <p style="text-align:center;font: size 10px;;">
                The template is based on <a href="http://jonbarron.info/">Jon Barron</a>'s website.
              </p>
              <a href='https://clustrmaps.com/site/1aokh'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=475b9e&w=340&t=tt&d=1Hy1olEvEwHbSO1bS0j16i-wjqkfkVfzM6ADMGK3X7k&co=ffffff&ct=ba2b2b'/></a>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
